import OpenAI from 'openai'
import { ChatCompletionMessageParam } from 'openai/resources/chat/completions'

/**
 * OpenAI æœåŠ¡ç±»
 * å°è£… OpenAI API è°ƒç”¨é€»è¾‘
 */
export class OpenAIService {
  private client: OpenAI
  private defaultModel: string
  private defaultTemperature: number
  private defaultMaxTokens: number

  constructor(options: {
    apiKey: string
    model?: string
    temperature?: number
    maxTokens?: number
  }) {
    this.client = new OpenAI({
      apiKey: options.apiKey,
    })
    
    this.defaultModel = options.model || 'gpt-3.5-turbo'
    this.defaultTemperature = options.temperature || 0.7
    this.defaultMaxTokens = options.maxTokens || 1000
  }

  /**
   * ç”ŸæˆèŠå¤©å›å¤
   */
  async chat(options: {
    messages: ChatCompletionMessageParam[]
    model?: string
    temperature?: number
    maxTokens?: number
    stream?: boolean
  }) {
    try {
      const {
        messages,
        model = this.defaultModel,
        temperature = this.defaultTemperature,
        maxTokens = this.defaultMaxTokens,
        stream = false
      } = options

      console.log(`ğŸ¤– OpenAI Chat: ${model}, messages: ${messages.length}`)

      const response = await this.client.chat.completions.create({
        model,
        messages,
        temperature,
        max_tokens: maxTokens,
        stream,
      })

      if (stream) {
        return response
      } else {
        const completion = response as OpenAI.Chat.Completions.ChatCompletion
        const content = completion.choices[0]?.message?.content || ''
        
        console.log(`âœ… OpenAI å“åº”ç”ŸæˆæˆåŠŸ, tokens: ${completion.usage?.total_tokens}`)
        
        return {
          content,
          usage: completion.usage,
          model: completion.model,
        }
      }
    } catch (error: any) {
      console.error('âŒ OpenAI API è°ƒç”¨å¤±è´¥:', error.message)
      throw new Error(`OpenAI API é”™è¯¯: ${error.message}`)
    }
  }

  /**
   * æµå¼ç”Ÿæˆ
   */
  async streamChat(options: {
    messages: ChatCompletionMessageParam[]
    model?: string
    temperature?: number
    maxTokens?: number
    onChunk?: (chunk: string) => void
  }) {
    try {
      const {
        messages,
        model = this.defaultModel,
        temperature = this.defaultTemperature,
        maxTokens = this.defaultMaxTokens,
        onChunk
      } = options

      console.log(`ğŸ”„ OpenAI Stream Chat: ${model}`)

      const stream = await this.client.chat.completions.create({
        model,
        messages,
        temperature,
        max_tokens: maxTokens,
        stream: true,
      })

      let fullContent = ''
      
      for await (const chunk of stream) {
        const content = chunk.choices[0]?.delta?.content || ''
        if (content) {
          fullContent += content
          onChunk?.(content)
        }
      }

      console.log(`âœ… OpenAI æµå¼å“åº”å®Œæˆ, æ€»é•¿åº¦: ${fullContent.length}`)
      
      return fullContent
    } catch (error: any) {
      console.error('âŒ OpenAI æµå¼è°ƒç”¨å¤±è´¥:', error.message)
      throw new Error(`OpenAI æµå¼ API é”™è¯¯: ${error.message}`)
    }
  }

  /**
   * è§£æä½ç½®ä¿¡æ¯
   */
  async parseLocation(userInput: string): Promise<{
    city: string
    country: string
    confidence: number
  }> {
    try {
      const messages: ChatCompletionMessageParam[] = [
        {
          role: 'system',
          content: `ä½ æ˜¯ä¸€ä¸ªåœ°ç†ä½ç½®è§£æä¸“å®¶ã€‚ä»ç”¨æˆ·è¾“å…¥ä¸­æå–åŸå¸‚å’Œå›½å®¶ä¿¡æ¯ï¼Œè¿”å›JSONæ ¼å¼ï¼š
{
  "city": "åŸå¸‚å",
  "country": "å›½å®¶ä»£ç ï¼ˆå¦‚CNã€USç­‰ï¼‰",
  "confidence": 0.9
}

å¦‚æœæ— æ³•ç¡®å®šä½ç½®ï¼Œè¯·è®¾ç½®confidenceä¸º0å¹¶è¯´æ˜åŸå› ã€‚`
        },
        {
          role: 'user',
          content: `è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–ä½ç½®ä¿¡æ¯ï¼š"${userInput}"`
        }
      ]

      const response = await this.chat({ messages, temperature: 0.3 })
      
      try {
        const parsed = JSON.parse(response.content)
        return {
          city: parsed.city || '',
          country: parsed.country || 'CN',
          confidence: parsed.confidence || 0
        }
      } catch (parseError) {
        console.error('è§£æä½ç½®JSONå¤±è´¥:', parseError)
        return {
          city: '',
          country: 'CN',
          confidence: 0
        }
      }
    } catch (error: any) {
      console.error('ä½ç½®è§£æå¤±è´¥:', error.message)
      return {
        city: '',
        country: 'CN',
        confidence: 0
      }
    }
  }

  /**
   * æ ¼å¼åŒ–å¤©æ°”å“åº”
   */
  async formatWeatherResponse(options: {
    weatherData: any
    originalQuery: string
  }): Promise<string> {
    try {
      const { weatherData, originalQuery } = options

      const messages: ChatCompletionMessageParam[] = [
        {
          role: 'system',
          content: `ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„å¤©æ°”åŠ©æ‰‹ã€‚è¯·å°†å¤©æ°”æ•°æ®æ ¼å¼åŒ–ä¸ºè‡ªç„¶ã€å‹å¥½çš„ä¸­æ–‡å›å¤ã€‚

è¦æ±‚ï¼š
1. ä½¿ç”¨å‹å¥½ã€è‡ªç„¶çš„ä¸­æ–‡
2. åŒ…å«å…³é”®å¤©æ°”ä¿¡æ¯ï¼šæ¸©åº¦ã€å¤©æ°”çŠ¶å†µã€æ¹¿åº¦ã€é£é€Ÿç­‰
3. å¦‚æœæœ‰æœ‰ç”¨çš„å»ºè®®ï¼ˆå¦‚ç©¿è¡£ã€å‡ºè¡Œï¼‰ï¼Œè¯·åŒ…å«
4. ä¿æŒç®€æ´ä½†ä¿¡æ¯å®Œæ•´
5. å¦‚æœæ²¡æœ‰å¤©æ°”æ•°æ®ï¼Œè¯·ç¤¼è²Œåœ°è¯´æ˜æ— æ³•è·å–è¯¥åœ°åŒºçš„å¤©æ°”ä¿¡æ¯`
        },
        {
          role: 'user',
          content: `è¯·å°†ä»¥ä¸‹å¤©æ°”æ•°æ®æ ¼å¼åŒ–ä¸ºå‹å¥½çš„ä¸­æ–‡å›å¤ï¼š

å¤©æ°”æ•°æ®ï¼š${JSON.stringify(weatherData, null, 2)}
åŸå§‹æŸ¥è¯¢ï¼š${originalQuery}`
        }
      ]

      const response = await this.chat({
        messages,
        temperature: 0.8,
        maxTokens: 500
      })

      return response.content
    } catch (error: any) {
      console.error('æ ¼å¼åŒ–å¤©æ°”å“åº”å¤±è´¥:', error.message)
      return 'æŠ±æ­‰ï¼Œæ— æ³•ç”Ÿæˆå¤©æ°”å›å¤ï¼Œè¯·ç¨åé‡è¯•ã€‚'
    }
  }

  /**
   * æ£€æŸ¥æœåŠ¡å¥åº·çŠ¶æ€
   */
  async checkHealth(): Promise<boolean> {
    try {
      await this.chat({
        messages: [{ role: 'user', content: 'Hello' }],
        maxTokens: 5
      })
      return true
    } catch (error) {
      console.error('OpenAI æœåŠ¡å¥åº·æ£€æŸ¥å¤±è´¥:', error)
      return false
    }
  }
}

// å¯¼å‡º OpenAI æœåŠ¡å®ä¾‹
export const openaiService = new OpenAIService({
  apiKey: process.env.OPENAI_API_KEY!,
  model: 'gpt-3.5-turbo',
  temperature: 0.7,
  maxTokens: 1000,
})
